[
    {
        "question": "According to the text, what specific GPU throughput scaling factor does the GB200 NVL72 achieve compared to a single eight-GPU system, and which specific protocol feature is attributed to delivering 4X bandwidth efficiency within the NVLink Switch infrastructure?",
        "correct_ans": "The GB200 NVL72 supports 9X the GPU throughput compared to a single eight-GPU system, and the 4X bandwidth efficiency is delivered by the new NVIDIA Scalable Hierarchical Aggregation and Reduction Protocol (SHARP)\u2122 with FP8 support.",
        "actual_content": "Fifth-Generation NVLink and NVLink Switch\n\nUnlocking the full potential of exascale computing and trillion-parameter Al models hinges on the need for swift, seamless communication among every GPU within a server cluster. The fifth generation of NVLink can scale up to 576 GPUs to accelerate performance for trillion- and multi-trillion-parameter Al models thanks to the NVLink Switch ASIC and switches built with it. Fifth-generation NVLink doubles the performance of fourth- generation NVLink in NVIDIA Hopper. While the new NVLink in Blackwell GPUs also uses two high-speed differential pairs in each direction to form a single link as in the Hopper GPU, NVIDIA Blackwell doubles the effective bandwidth per link to 50 GB/sec in each direction.\n\nBlackwell GPUs include 18 fifth-generation NVLink links to provide 1.8 TB/sec total bandwidth, 900 GB/sec in each direction. 1.8TB/s of bidirectional throughput per GPU is over 14X the bandwidth of PCle Gen5, ensuring high-speed communication for today\u2019s most complex large models. That's nearly seven petabytes of data transfer in an hour from one GPU, or more data than 18 years of streaming 4K movies, or the entire Internet bandwidth processed by just 11 Blackwell GPUs.\n\nThe NVIDIA NVLink Switch enables 130TB/s GPU bandwidth in one 72 GPU NVLink domain (NVL72) for model parallelism, and delivers 4X bandwidth efficiency with new NVIDIA Scalable Hierarchical Aggregation and Reduction Protocol (SHARP)\u2122 FP8 support. NVLink and NVLink Switch used together support clusters beyond a single server at the same impressive 1.8 TB/s interconnect. Multi-server clusters using NVLink Switch can scale GPU communications in balance with the increased computing, enabling the GB200 NVL72 to support 9X the GPU throughput as compared to a single eight-GPU system.\n\nThe NVLink Switch works with the NVIDIA Unified Fabric Manager (UFM\u2019) to offer production-proven management for the NVLink compute fabric.\n\nNVIDIA Blackwell Architecture Technical Brief\n\nNVIDIA Blackwell Architectural Innovations"
    },
    {
        "question": "Identify the platform described as offering drop-in replacement compatibility for existing HGX H100 infrastructure and calculate the difference in FP32 performance (in teraFLOPS) between this platform and the HGX B200.",
        "correct_ans": "The platform is the HGX B100. The difference in FP32 performance is 20 teraFLOPS (The HGX B200 provides 80 teraFLOPS, while the HGX B100 provides 60 teraFLOPS).",
        "actual_content": "The NVIDIA Blackwell HGX B200 and HGX B100 are advanced computing platforms designed for generative AI, data analytics, and high-performance computing. These platforms incorporate the same groundbreaking advancements as their predecessors but extend the HGX series to include Blackwell GPUs.\n\n**Key Features and Performance Metrics:**\n\n- **HGX B200:** This platform is based on an eight-Blackwell GPU baseboard and delivers 144 petaFLOPs of AI performance. It offers the best performance and Total Cost of Ownership (TCO) for x86 scale-up platforms and infrastructure, outperforming the HGX H100 by 15X in performance and 12X in TCO. Each GPU in the HGX B200 is configurable up to 1000 Watts.\n\n- **HGX B100:** Also based on an eight-Blackwell GPU B100 baseboard, it delivers 112 petaFLOPs of AI performance. The HGX B100 is designed for the fastest time to deployment with drop-in replacement compatibility for existing HGX H100 infrastructure. Each GPU in this platform is configurable up to 700 Watts.\n\n**Detailed Specifications:**\n\n### Computational Performance:\n\n- **FP4 Tensor Core:** The HGX B200 offers 144 PetaFLOPS, while the HGX B100 provides 112 PetaFLOPS.\n- **FP8/FP6/INT8:** The HGX B200 delivers 72 PetaFLOPS, and the HGX B100 delivers 56 PetaFLOPS.\n- **TF32 Tensor Core:** The HGX B200 offers 2.2 petaFLOPS, and the HGX B100 offers 1.8 petaFLOPS.\n- **FP32:** The HGX B200 provides 80 teraFLOPS, and the HGX B100 provides 60 teraFLOPS.\n- **FP64 Tensor Core:** The HGX B200 offers 40 teraFLOPS, and the HGX B100 offers 30 teraFLOPS.\n\n### Memory and Bandwidth:\n\n- **Fast Memory/Aggregate Memory:** Up to 1.5 TB for both platforms.\n- **Aggregate Memory Bandwidth:** Up to 64 TB/s for both platforms.\n- **Aggregate NVLink Bandwidth:** 14.4 TB/s for both platforms.\n- **GPU Memory:** Up to 192 GB HBM3e with up to 8 TB/s bandwidth.\n\n### Power and Thermal Design:\n\n- **Max Thermal Design Power (TDP):** 1,000W for the HGX B200 and 700W for the HGX B100.\n\n### Connectivity:\n\n- **NVLink Bandwidth:** 1.8 TB/s for both platforms.\n- **PCIe Gen6 Bandwidth:** 256 GB/s for both platforms.\n\n### Server Options:\n\nBoth platforms are available in NVIDIA HGX B200 and HGX B100 partner and NVIDIA-Certified Systems configurations, supporting 8 GPUs.\n\n**Summary:**\n\nThe NVIDIA Blackwell HGX B200 and HGX B100 platforms represent a significant advancement in computing performance, especially for AI and HPC applications. With their enhanced performance metrics, improved TCO, and compatibility with existing infrastructures, these platforms are poised to accelerate a wide range of computational tasks. The detailed specifications highlight the capabilities of these platforms, including their computational performance, memory, bandwidth, power consumption, and connectivity options. These features make the HGX B200 and HGX B100 highly suitable for applications requiring high-performance computing, such as generative AI, data analytics, and scientific simulations."
    },
    {
        "question": "According to the figure caption provided in the text, what specific combination of processing units comprises the NVIDIA GB200 Superchip?",
        "correct_ans": "Two Blackwell GPUs and one Grace CPU",
        "actual_content": "NVIDIA Blackwell GPU and Superchip Overview\n\nLarge language models (LLMs) require immense computational power for real-time performance. The computational demands of LLMs also translate into higher energy consumption as more and more memory, accelerators, and servers are required to fit, train, and infer from these models. Organizations aiming to deploy LLMs for real-time inference must grapple with these challenges.\n\nThe NVIDIA Blackwell architecture and portfolio of products are designed to address the needs of ever-increasing Al model sizes and parameters with a long list of new innovations, including a new second-generation Transformer Engine.\n\nThe NVIDIA Blackwell architecture was named to honor David H. Blackwell, an amazing and inspiring American mathematician and statistician known for the Rao-Blackwell Theorem, and many contributions and advancements in probability theory, game theory, statistics, and dynamic programming.\n\nWith NVIDIA Blackwell products, every enterprise can use and deploy state-of-the-art LLMs with affordable economics, optimizing their business with the benefits of generative Al. At the same time, NVIDIA Blackwell products enable the next era of generative Al models, supporting multi-trillion parameter models with real-time performance, something unattainable without Blackwell\u2019s innovations.\n\nNVIDIA Blackwell Architecture Technical Brief\n\nIntroduction\n\nFigure 1. NVIDIA GB200 Superchip Incl. Two Blackwell GPUs and One Grace CPU\n\nNVIDIA Blackwell Architecture Technical Brief\n\nNVIDIA Blackwell Architectural Innovations"
    },
    {
        "question": "According to the detailed performance notes for the GB200 Database Join Query, what specific workload characteristics (query derivation and compression types) were used to establish the benchmark where NVIDIA Blackwell performs 18X faster than CPUs and 6X faster than NVIDIA H100 Tensor Core GPUs?",
        "correct_ans": "The benchmark was established using a database join and aggregation workload derived from the TPC-H Q4 query, utilizing Snappy and Deflate compression.",
        "actual_content": "Decompression Engine\n\nData analytics and database workflows have traditionally been slow and cumbersome, relying on CPUs for compute. Accelerated data science can dramatically boost the performance of end-to-end analytics, speeding up value generation and time to insights while reducing cost. Databases, including Apache Spark, play critical roles in handling, processing, and analyzing large volumes of data for data analytics. Blackwell\u2019s new dedicated Decompression Engine can decompress data at a rate of up to 800GB/s, and in combination with 8TB/s of HBM3e (High Bandwidth Memory) using one GPU in GB200 and the Grace CPU\u2019s high-speed NVLink-C2C (Chip-to-Chip) interconnect, accelerate the full pipeline of database queries for the highest performance in data analytics and data science. With support for the latest compression formats, such as LZ4, Snappy, and Deflate, NVIDIA Blackwell performs 18X faster than CPUs and 6X faster than NVIDIA H100 Tensor Core GPUs for query benchmarks.\n\n100\n\n90 \u2014\u2014 80 70 60 50 18X 40 Queries per Second 30 20 10 5 \u2014 x86 HGX H100 GB200 NVL72 8)\n\nProjected performance subject to change. Database join and aggregation workload with Snappy / Deflate compression derived from TPC-H Q4 query. Custom query implementations for x86, HGX H100 single GPU, and single GPU from GB200 Superchip.\n\nFigure 3. GB200 Database Join Query Using Decompression Engine\n\nNVIDIA Blackwell Architecture Technical Brief\n\nNVIDIA Blackwell Architectural Innovations"
    },
    {
        "question": "According to the text, how does the Blackwell architecture overcome the physical manufacturing limit regarding the maximum possible size of a single die to increase computing power, and what specific technical specification allows the resulting components to function as one fully coherent GPU?",
        "correct_ans": "The architecture overcomes the physical limit by merging two dies that are each already at the maximum reticle size limit. To function as a coherent, unified GPU, these two dies are connected by a 10 terabyte-per-second (TB/s) chip-to-chip NVIDIA High-Bandwidth Interface (NV-HBI).",
        "actual_content": "A New Class of Al Superchip\n\nBuilt with 208 billion transistors, more than 2.5x the amount of transistors in NVIDIA Hopper GPUs, and using 's 4NP process tailored for NVIDIA, Blackwell is the largest GPU ever built. NVIDIA Blackwell achieves the highest compute ever on a single chip, 20 petaFLOPS.\n\nNVIDIA Blackwell Architecture Technical Brief\n\nNVIDIA Blackwell Architectural Innovations\n\nThis architecture is able to incorporate a significant amount of computing power by merging two dies into a single, unified GPU. Each of the two dies are the largest die possible within the limits of reticle size, as big as can possibly be built today. The two dies are connected and unified with a single 10 terabyte-per-second (TB/s) chip-to-chip NVIDIA High-Bandwidth Interface (NV-HBI), providing one fully coherent, unified GPU.\n\nThe Blackwell architecture is much more than a chip with high floating-point operations per second (FLOPS) computational rates. It continues to build upon and benefit from NVIDIA's rich ecosystem of development tools, CUDA-X\u2122 libraries, over four million developers, and over 3,000 applications scaling performance across thousands of nodes."
    },
    {
        "question": "Based strictly on the provided text, which four specific trademarks or registered trademarks of NVIDIA Corporation are listed that do not begin with the word \"NVIDIA\"?",
        "correct_ans": "GeForce, Quadro, CUDA, and GeForce RTX",
        "actual_content": "Trademarks\n\nNVIDIA, the NVIDIA logo, NVIDIA CUDA, NVIDIA Omniverse, NVIDIA RTX, NVIDIA Tesla, NVIDIA Turing, NVIDIA Volta, NVIDIA Jetson AGX Xavier, NVIDIA DGX, NVIDIA HGX, NVIDIA EGX, NVIDIA CUDA-X, NVIDIA GPU Cloud, GeForce, Quadro, CUDA, GeForce RTX, NVIDIA NVLink, NVIDIA NVSwitch, NVIDIA DGX POD, NVIDIA DGX SuperPOD, and NVIDIA TensorRT, are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.\n\nCopyright \u00a9 2024 NVIDIA Corporation. All rights reserved.\n\nNVIDIA Blackwell Architecture Technical Brief\n\nNotice\n\n24"
    },
    {
        "question": "According to the projected performance details at a 32,768 GPU scale, what specific networking bandwidth and cooling configuration distinguishes the GB200 NVL72 cluster from the HGX H100 cluster against which it is compared?",
        "correct_ans": "The GB200 NVL72 cluster utilizes an 800G IB (InfiniBand) network and is liquid-cooled, whereas the HGX H100 cluster utilizes a 400G IB network and is air-cooled.",
        "actual_content": "Next-Level Al Training Performance\n\nGB200 includes a faster Transformer Engine featuring FP8 precision and delivers 4X faster training performance for large language models like GPT-MoE-1.8T compared to the NVIDIA Hopper GPU generation. The performance boost provides a 9X reduction in rack space and a 3.5X reduction in TCO and energy usage. This breakthrough is complemented by the fifth-generation NVLink (which enables 1.8 TB/s of GPU-to-GPU interconnect and a larger 72-GPU NVLink domain), InfiniBand networking, and NVIDIA Magnum IO\u2122 software. Together, these ensure efficient scalability for enterprises and facilitate the implementation of extensive GPU computing clusters.\n\n4x Ww x Speedup Over H100 PO x Ix OX HGX H100 GB200 NVL72\n\nProjected performance subject to change. 32,768 GPU scale, 4,096x HGX H100 air-cooled cluster: 400G IB network, 456x GB200 NVL72 liquid-cooled cluster: 800G IB network"
    },
    {
        "question": "By cross-referencing the descriptive text with the specifications in Figure 2, what is the specific throughput speed associated with the architectural component designed to fuel data analytics applications?",
        "correct_ans": "800 GB/sec",
        "actual_content": "NVIDIA Blackwell Architectural Innovations\n\nThe Blackwell architecture introduces groundbreaking advancements for generative Al and accelerated computing. The incorporation of a new second-generation Transformer Engine, alongside faster and wider interconnects, propels the data center into a new era, with orders of magnitude more performance compared to the previous architecture generation.\n\nFurther advances in technology raise the level of security for real-time generative Al inference at scale without compromising performance. And NVIDIA Blackwell\u2019s new Decompression Engine combined with libraries delivers unparalleled database performance to fuel data analytics applications. NVIDIA Blackwell\u2019s multiple advancements build upon generations of accelerated computing technologies to define the next chapter of generative Al with unparalleled performance, efficiency, and scale.\n\nls x Al SUPERCHIP 2.4 GEN TRANSFORMER ENGINE 5th GENERATION NVLINK 208B Transistors FP4/FP6 Tensor Core Scales to 576 GPUs \u2014_\u2014 \\ _\u2014 \u00a5 ~ = . ene, DECOMPRESSION ENGINE RAS ENGINE SECURE Al 800 GB/sec 100% In-System Self-Test Full Performance Encryption & TEE\n\nFigure 2. NVIDIA Blackwell Architecture\u2019s Technological Breakthroughs"
    },
    {
        "question": "According to the text, which specific networking platform forms the foundation of the AI compute fabric, and what is the magnitude of its scaling improvement over the NVIDIA Quantum-2 generation when utilizing a two-level fat tree topology?",
        "correct_ans": "Quantum-X800 InfiniBand, which offers 5X higher scaling capability than the previous NVIDIA Quantum-2 generation.",
        "actual_content": "Lower is Better\n\nHGXH100 \u00a7\u00a7\u00a7GB200 NVL72\n\nTotal Cost of Ownership Energy Use\n\nTCO and energy savings for 65 racks eight-way HGX H100 air-cooled versus 1 rack GB200 NLV72 liquid-cooled with equivalent performance on GPT-MOoE-1.8T real-time inference throughput.\n\nFigure 7. 25X Lower Energy Use and TCO\n\nAccelerated Networking Platforms for Generative Al\n\nGB200 NVL72, acting as a single, extremely powerful unit of computing, requires robust networking to achieve optimal application performance. Paired with NVIDIA Quantum- X800 InfiniBand, Spectrum-X800 Ethernet, and BlueField-3 DPUs, GB200 delivers unprecedented levels of performance, efficiency, and security in massive-scale Al data centers.\n\nQuantum-X800 InfiniBand forms the foundation of the Al compute fabric, capable of scaling beyond 10,000 GPU in a two-level fat tree topology, which is 5X higher than the previous NVIDIA Quantum-2 generation. NVIDIA Spectrum-X800 and BlueField-3 DPU platforms are used to scale across the data center, providing accelerated GPU access to data, secure cloud multi-tenancy, and efficient data center operations.\n\nNVIDIA Blackwell Architecture Technical Brief\n\nNVIDIA Blackwell HGX"
    },
    {
        "question": "According to the sub-sections listed under Appendix A, which specific model configuration is cited when discussing the 'Impact of Chunk Size'?",
        "correct_ans": "GPT 1.8T MoE Model",
        "actual_content": ">\n\nNVIDIA.\n\nNVIDIA Blackwell Architecture Technical Brief\n\nPowering the New Era of Generative Al and Accelerated Computing\n\nTable of Contents\n\nPioneering Al INMOVATION......csssccsessssssscsesseeseessessesenscessncsaesseseeseessnssaeseeseesuesnssassaecnesnesansasenscnesnesaesoseonss 4 NVIDIA Blackwell GPU and Superchip Overview......sssssssssssssscsessesseessessassseseseessnssnssnseeesnesaeesesonss 4 NVIDIA Blackwell Architectural INNOVATIONS. .......cssscsssssseesssessesesseeseessnssaesnesnesesenssnsensenesnesaessssonss 6 A New Class of Al SUPCr Chip... eee ececeeececscsesseesesenesenesesesssesesentesensseassssneasstassensseataenssesesteneaesceneeeneaes 6 Second-Generation Transformer Engine ..0..........ccccccccscsesssesesesesseseseeessesesessssnsseseseneseseessseneseseeseneseacees 7 Performant Confidential Computing ANd SECUrE Al oun... ceeeececseseseseeseseeeeseseseseeencscaeeteeseseeenenesees 7 Fifth-Generation NVLINK ANd NVLIMNK SWITCH oo... eee eeeeceeeeseeseeseeeeeeeeeeneeeceeeneeecnecneeneereneeneeneeeeneenees 8 DECOMPLeSSION ENGIN... eeeececececeseseseseseseseesesesesesescucecssesesesesescsesessacsescsesesecsescseseneseeseataeseseeeseseeeeeeatatees 9 RAS Elie ...n..n.eeeecesecesesesesesesesceeesescsescseseucsecacacsescsesesesccssscscsesescsssseseseseneessssaesescsesesesenssescseseeeeeseetseesseeeeeeeeees 10 NVIDIA GB200 Superchip and GB200 NVL72.....sssssssssssssessscessesseseseessseesessecesaeeeseeseseeeesnseesaseesesaess 11 Real-Time Inference for the Next Generation of Large Language Models ..............ccseeeeee 14 Next-Level Al Training Perfor mance............ccccccsessssesessssssesesssesesesessacsesnssssescsusseseaesnencseatsnsneaeaeateeeeacens 16 Accelerating Data Processing and Physics-Based Simulation..............ccccccseseescseseseeneseeeeeeeeeeees 17 Sustainable COMPUTING o.0...... eee eceeeesecessesesesseseseeessesesescensnesescesueacseenssesssesnsnsssseseenessasstansnsnsseeneseatsesneneneaeanes 17 Accelerated Networking Platforms for Generative Alan... cccccccccsscesesesssscseseseeseneseseeeneseaeenees 18 NVIDIA Blackwell HGX......cssssssssessssesessesersessseecseeesscseseeseseeesaeeececsesecaeseseeseseeseseenecaeeeceseeneesesasenaneesenoess 19 NVIDIA Blackwell Architecture\u2019s Role in the Age of Generative Al......ssssssesssssssssssessssssees 21 APPCNX A... cscssssssssesesssncsaesnsseeseessnssassnsenesnesaesnsenscnssnssaesassnesneeanenaesassaesnssaesanseesenssnesaesaseeesnessnssaesnsseesaseass 22 Advanced Parallelism Techniques in Al Inference for Trillion-Parameter Models............... 22 Parallelism Techniques in Al INFEreNCe ou... eee ceecccccsscscsescsesesescsescsescscsesestscetscetenesesessscscseacacacacaces 22 Combining Parallelism TeCHniques................ecececsesesessssesesessesesesessenesesesesseaeseessesesnsneseseacsnsnsseatanenenees 22 Maximizing Throughput and Managing Operational Phases qu... eceescecesseesesesteneneseenees 22 INFlight Batching ANd CHUNKING 0.0.0... eseeeseseessseseeesseseseseesencseseeensseseenessaesesnssesesestenssencaeeneeees 23 Impact of Chunk Size on GPT 1.8T MOE Model... cecccecesceecssesesseseseseetsseseseseeneneseensneneaeenenees 23 Oo) a el 0) 0) 9 ee 23\n\nNVIDIA Blackwell Architecture Technical Brief\n\nii"
    }
]